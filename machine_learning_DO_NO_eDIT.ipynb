{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) that is designed to handle the problem of vanishing gradient in traditional RNNs. LSTM networks can learn long-term dependencies in sequential data and are widely used in various applications such as speech recognition, language modeling, and time series forecasting.\n",
    "\n",
    "The key feature of an LSTM cell is its ability to selectively remember and forget information using its memory cell and gates. The memory cell stores the previous hidden state and carries it forward in time. The input gate determines how much new information is allowed into the memory cell, and the forget gate decides how much of the previous memory to retain or forget. Finally, the output gate controls how much of the current memory cell state is used to generate the output. The gates are controlled by sigmoid and element-wise multiplication operations, and the memory cell is updated using a tanh function.\n",
    "\n",
    "To train an LSTM model, we need to define the architecture, loss function, and optimization method. The input to the model is a sequence of vectors, and the output is a sequence of predicted values. The architecture can have multiple LSTM layers with varying hidden units, dropout, and activation functions. The loss function can be Mean Squared Error (MSE) or Binary Cross Entropy (BCE), depending on the type of problem. The optimization method can be Stochastic Gradient Descent (SGD), Adam, or other variations.\n",
    "\n",
    "To make our LSTM model have the highest accuracy, we can follow the following experimental procedure:\n",
    "\n",
    "1. Preprocess the data: The input data should be normalized, and the sequences should be padded or truncated to a fixed length. We can also use techniques like data augmentation or feature engineering to enhance the data quality.\n",
    "\n",
    "2. Split the data: We need to split the data into training, validation, and test sets. The training set is used to update the model parameters, the validation set is used to monitor the model performance and avoid overfitting, and the test set is used to evaluate the final model accuracy.\n",
    "\n",
    "3. Define the architecture: We can start with a simple LSTM model and gradually increase the complexity by adding more layers or hidden units. We can also use techniques like early stopping, learning rate scheduling, or batch normalization to improve the model performance.\n",
    "\n",
    "4. Train the model: We can use the Adam optimizer with a learning rate of 0.001, and a batch size of 32. We can train the model for 50 epochs and monitor the validation loss to avoid overfitting. We can also use techniques like gradient clipping or weight decay to prevent exploding gradients.\n",
    "\n",
    "5. Evaluate the model: We can use the test set to evaluate the final model accuracy. We can compute metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or Mean Absolute Error (MAE) to assess the model performance. We can also visualize the predicted values against the ground truth to gain insights into the model behavior.\n",
    "\n",
    "6. By following these steps, we can create an LSTM model that achieves high accuracy and generalizes well to new data. The key to success is to experiment with different architectures, optimization methods, and hyperparameters and choose the ones that work best for our specific problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model\n",
    "1. Make sure all packages are installed (WINDOWS)\n",
    "\n",
    "*    pip install numpy\n",
    "*   pip install pandas\n",
    "*   pip install os-sys\n",
    "*    pip install matplotlib\n",
    "*   pip install seaborn\n",
    "*   pip install scikit-learn\n",
    "*   pip install keras\n",
    "*   pip install tensorflow \n",
    "\n",
    "1. Make sure all packages are installed (MACS AND/OR ANACONDA ENVIRONMENT)\n",
    "*   conda  install numpy\n",
    "*   conda  install pandas\n",
    "*   conda  install os-sys\n",
    "*   conda  install matplotlib\n",
    "*   conda  install seaborn\n",
    "*   conda  install scikit-learn\n",
    "*   conda  install keras\n",
    "*   conda  install tensorflow \n",
    "\n",
    "Links:\n",
    "\n",
    "Kaggle Data: https://www.kaggle.com/harnoor343/fall-detection-accelerometer-data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Issues:\n",
    " * can I reshape before putting in LSTM layer :(\n",
    "\n",
    "Todo:\n",
    "* understand acceleration data comepared to previosu acceleration data\n",
    "* normalize it?\n",
    "* let's use fall data only first cases (), then all the cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN IF YOU ARE NOT USING WINDOWS AND/OR NOT USING ANACONDA\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install os-sys\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install keras\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored like this:\n",
    "['SubjectID', 'Device','ActivityID','TrialNo','Acc','Gyr']\n",
    "- The label (output) is the ActivityID\n",
    "    - These labels are specified in function get_activity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection():\n",
    "    #1: runfall, downSit, freeFall, runSit, walkFall, walkSit\n",
    "    #2: ActivityID labels\n",
    "    # create a new directory called 'normalized', ignore it if it already exists\n",
    "    os.makedirs('./normalized', exist_ok=True)\n",
    "\n",
    "    # import pickle file\n",
    "    df = pd.read_pickle('FallAllD2.pkl')\n",
    "    # convert all columns to float32\n",
    "    #df = df.astype(np.float32)\n",
    "\n",
    "    #display df\n",
    "    #print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def data_normalize(df): #CHECK IF NEEDED\n",
    "    # for each column in 'Accerlation', 'AccelerationY', 'AccelerationZ',\n",
    "    # add a new column that is the fft of the original column\n",
    "    for c in ['Acc']:\n",
    "        lst = []\n",
    "        for i in c:\n",
    "           lst.append(np.fft.fft(i))\n",
    "        df[c + '_fft'] = lst\n",
    "\n",
    "    # debug purposes\n",
    "    for c in df.columns:\n",
    "        print(c)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def data_categorical(df): #TODO: CHECK IF NEEDED\n",
    "    # convert column 'Device' to numerical data\n",
    "    df['Device'] = df['Device'].astype('category')\n",
    "    df['Device'] = df['Device'].cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "def data_split(df):\n",
    "    # split the data into features and labels\n",
    "    x = df[['SubjectID', 'Device','TrialNo','Acc_x','Acc_y','Acc_z', 'Gyr_x', 'Gyr_y', 'Gyr_z']]\n",
    "    y = df['ActivityID']\n",
    "\n",
    "    #Spliting Data\n",
    "    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x,y,test_size = 0.3)\n",
    "\n",
    "    # reshape training and testing data\n",
    "    y_train = np.array(y_train).reshape(-1,1) # (-1,1) because our data has a single feature, and a 'n' amount of rows\n",
    "    y_test = np.array(y_test).reshape(-1,1) # (-1,1) bec  ause our data has a single feature, and a 'n' amount of rows\n",
    "    x_test = np.array(x_test).reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def model_create(x_train):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(128, input_shape=(x_train.shape[1], 1)))\n",
    "  model.add(Dropout(0.2))\n",
    "  # there are 135 different activities, so we need 135 neurons in the output layer\n",
    "  model.add(Dense(136, activation='softmax'))\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def train_and_accurary_model(model, x_train, x_test, y_train, y_test):\n",
    "  #reshape the features for the LSTM layer (I REALLY WANT TO FIX UGHHHG)\n",
    "  x_train = np.array(x_train).reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "\n",
    "  # train the model\n",
    "  model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "  # evaluate the model\n",
    "  test_loss, test_Acc = model.evaluate(x_test, y_test)\n",
    "  print('Test accuracy:', test_Acc)\n",
    "  model.summary()\n",
    "\n",
    "  # make a confusion matrix out of the training and test data\n",
    "\n",
    "  return model\n",
    "\n",
    "def model_save(model):\n",
    "  # save the model\n",
    "  model.save('saved_model/my_model')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = data_normalize(df) #Don't know if I need to normalize\n",
    "df = data_categorical(df) #check for 'Device'\n",
    "print('data collected')\n",
    "print(df.shape)\n",
    "# convert all columns to float32\n",
    "df = df.astype('float32')\n",
    "x_train, x_test, y_train, y_test = data_split(df)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "print(\"Data Split Done\")\n",
    "model = model_create(x_train)\n",
    "print(\"Model Created\")\n",
    "model = train_and_accurary_model(model, x_train, x_test, y_train, y_test)\n",
    "print(\"Model Trained\")\n",
    "model = model_save(model)\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions Below Not Using. Don't Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging function\n",
    "def data_plot(df):\n",
    "    #debugging function. does not need to be called, or return anything\n",
    "\n",
    "    # plot the first 100 column 'AccelerationX' and 'AccelerationX_fft'\n",
    "    df[['Acc'[0]], ['Acc'[1]], ['Acc'[2]]].iloc[:100].plot()\n",
    "\n",
    "    # Versus Head (the same)\n",
    "    #df[['AccelerationX_fft', 'AccelerationY_fft', 'AccelerationZ_fft']].head(100).plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(num = 0):\n",
    "    # a dictionary that maps the case number (ActivityID) with stirng label\n",
    "    activity_dict = {\n",
    "    101: 'Fall F, walking, trip',\n",
    "    102: 'Fall F, walking, trip, rec.',\n",
    "    103: 'Fall F, walking, slip',\n",
    "    104: 'Fall F, walking, slip, rec.',\n",
    "    105: 'Fall F, walking, slip, rot.',\n",
    "    106: 'Fall F, walking, slip, rot., rec.',\n",
    "    107: 'Fall B, walking, slip',\n",
    "    108: 'Fall B, walking, slip, rec.',\n",
    "    109: 'Fall B, walking, slip, rot.',\n",
    "    110: 'Fall B, walking, slip rot., rec.',\n",
    "    111: 'Fall F, walking, syncope',\n",
    "    112: 'Fall B, walking, syncope',\n",
    "    113: 'Fall L, walking, syncope',\n",
    "    114: 'Fall, syncope, table',\n",
    "    115: 'Fall F, try sit',\n",
    "    116: 'Fall F, try sit, rec.',\n",
    "    117: 'Fall B, try sit',\n",
    "    118: 'Fall B, try sit, rec.',\n",
    "    119: 'Fall L, try sit',\n",
    "    120: 'Fall L, try sit, rec.',\n",
    "    121: 'Fall F, jog, trip',\n",
    "    122: 'Fall F, jog, trip, rec.',\n",
    "    123: 'Fall F, jog, slip',\n",
    "    124: 'Fall F, jog, slip, rev.',\n",
    "    125: 'Fall F, jog, slip, rot.',\n",
    "    126: 'Fall F, jog, slip, rot., rec.',\n",
    "    127: 'Fall L, bed',\n",
    "    128: 'Fall L, bed, rec.',\n",
    "    129: 'Fall F, chair, syncope',\n",
    "    130: 'Fall B, chair, syncope',\n",
    "    131: 'Fall L, chair, syncope',\n",
    "    132: 'Fall F, syncope',\n",
    "    133: 'Fall B, syncope',\n",
    "    134: 'Fall L, syncope',\n",
    "    135: 'Fall, syncope, slide over a wall',\n",
    "    1: 'Start clap hands',\n",
    "    2: 'Clap hands',\n",
    "    3: 'Stop clap hands',\n",
    "    4: 'Clap hands 1',\n",
    "    5: 'Start wave hands',\n",
    "    6: 'wave hands',\n",
    "    7: 'Stop wave hands',\n",
    "    8: 'Raising hand up',\n",
    "    9: 'Moving hand down',\n",
    "    10: 'Move hand up -> down',\n",
    "    11: 'Hand shaking',\n",
    "    12: 'Beating a table',\n",
    "    13: 'Sitting down',\n",
    "    14: 'Standing up',\n",
    "    15: 'Fail to stand up',\n",
    "    16: 'Lying down',\n",
    "    17: 'Turning while lying',\n",
    "    18: 'Rising up',\n",
    "    19: 'Start walking',\n",
    "    20: 'Walking slowly',\n",
    "    21: 'Stop walking',\n",
    "    22: 'Walking quickly',\n",
    "    23: 'Stumbling',\n",
    "    24: 'Jogging slowly',\n",
    "    25: 'Jogging quickly',\n",
    "    26: 'Jumping slightly',\n",
    "    27: 'Jumping strongly',\n",
    "    28: 'B...'\n",
    "    }\n",
    "    return activity_dict[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs the entire machine learning model\n",
    "def run():\n",
    "    num = int(input(\"Choose a number from 1-2 to run either kaggle data or fallAllID data: \"))\n",
    "    df = data_collection(num)\n",
    "    print(\"Data Collection Done\")\n",
    "    #df = data_normalize(df) #Don't know if I need to normalize\n",
    "    df = data_categorical(df) #check for 'Device'\n",
    "    x_train, x_test, y_train, y_test = data_split(df)\n",
    "    print(\"Data Split Done\")\n",
    "    model = model_create(x_train)\n",
    "    print(\"Model Created\")\n",
    "    model = train_and_accurary_model(model, x_train, y_train, x_test, y_test)\n",
    "    print(\"Model Trained\")\n",
    "    model = model_save(model)\n",
    "    print(\"Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "919ada50126e9b7679f5f5dbe18aa91f971b460080d3011effa906ebff41bb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
