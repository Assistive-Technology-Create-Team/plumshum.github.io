{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) that is designed to handle the problem of vanishing gradient in traditional RNNs. LSTM networks can learn long-term dependencies in sequential data and are widely used in various applications such as speech recognition, language modeling, and time series forecasting.\n",
    "\n",
    "The key feature of an LSTM cell is its ability to selectively remember and forget information using its memory cell and gates. The memory cell stores the previous hidden state and carries it forward in time. The input gate determines how much new information is allowed into the memory cell, and the forget gate decides how much of the previous memory to retain or forget. Finally, the output gate controls how much of the current memory cell state is used to generate the output. The gates are controlled by sigmoid and element-wise multiplication operations, and the memory cell is updated using a tanh function.\n",
    "\n",
    "To train an LSTM model, we need to define the architecture, loss function, and optimization method. The input to the model is a sequence of vectors, and the output is a sequence of predicted values. The architecture can have multiple LSTM layers with varying hidden units, dropout, and activation functions. The loss function can be Mean Squared Error (MSE) or Binary Cross Entropy (BCE), depending on the type of problem. The optimization method can be Stochastic Gradient Descent (SGD), Adam, or other variations.\n",
    "\n",
    "To make our LSTM model have the highest accuracy, we can follow the following experimental procedure:\n",
    "\n",
    "1. Preprocess the data: The input data should be normalized, and the sequences should be padded or truncated to a fixed length. We can also use techniques like data augmentation or feature engineering to enhance the data quality.\n",
    "\n",
    "2. Split the data: We need to split the data into training, validation, and test sets. The training set is used to update the model parameters, the validation set is used to monitor the model performance and avoid overfitting, and the test set is used to evaluate the final model accuracy.\n",
    "\n",
    "3. Define the architecture: We can start with a simple LSTM model and gradually increase the complexity by adding more layers or hidden units. We can also use techniques like early stopping, learning rate scheduling, or batch normalization to improve the model performance.\n",
    "\n",
    "4. Train the model: We can use the Adam optimizer with a learning rate of 0.001, and a batch size of 32. We can train the model for 50 epochs and monitor the validation loss to avoid overfitting. We can also use techniques like gradient clipping or weight decay to prevent exploding gradients.\n",
    "\n",
    "5. Evaluate the model: We can use the test set to evaluate the final model accuracy. We can compute metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or Mean Absolute Error (MAE) to assess the model performance. We can also visualize the predicted values against the ground truth to gain insights into the model behavior.\n",
    "\n",
    "6. By following these steps, we can create an LSTM model that achieves high accuracy and generalizes well to new data. The key to success is to experiment with different architectures, optimization methods, and hyperparameters and choose the ones that work best for our specific problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model\n",
    "1. Make sure all packages are installed (WINDOWS)\n",
    "\n",
    "*    pip install numpy\n",
    "*   pip install pandas\n",
    "*   pip install os-sys\n",
    "*    pip install matplotlib\n",
    "*   pip install seaborn\n",
    "*   pip install scikit-learn\n",
    "*   pip install keras\n",
    "*   pip install tensorflow \n",
    "\n",
    "1. Make sure all packages are installed (MACS AND/OR ANACONDA ENVIRONMENT)\n",
    "*   conda  install numpy\n",
    "*   conda  install pandas\n",
    "*   conda  install os-sys\n",
    "*   conda  install matplotlib\n",
    "*   conda  install seaborn\n",
    "*   conda  install scikit-learn\n",
    "*   conda  install keras\n",
    "*   conda  install tensorflow \n",
    "\n",
    "Links:\n",
    "\n",
    "Kaggle Data: https://www.kaggle.com/harnoor343/fall-detection-accelerometer-data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Issues:\n",
    " * can I reshape before putting in LSTM layer :(\n",
    "\n",
    "Todo:\n",
    "* understand acceleration data comepared to previosu acceleration data\n",
    "* normalize it?\n",
    "* let's use fall data only first cases (), then all the cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN IF YOU ARE NOT USING WINDOWS AND/OR NOT USING ANACONDA\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install os-sys\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install keras\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import sklearn.model_selection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs the entire machine learning model\n",
    "def run():\n",
    "    num = int(input(\"Choose a number from 1-2 to run either kaggle data or fallAllID data: \"))\n",
    "    df = data_collection(num)\n",
    "    print(\"Data Collection Done\")\n",
    "    #df = data_normalize(df) #Don't know if I need to normalize\n",
    "    df = data_categorical(df) #check for 'Device'\n",
    "    x_train, x_test, y_train, y_test = data_split(df)\n",
    "    print(\"Data Split Done\")\n",
    "    model = model_create(x_train)\n",
    "    print(\"Model Created\")\n",
    "    model = train_and_accurary_model(model, x_train, y_train, x_test, y_test)\n",
    "    print(\"Model Trained\")\n",
    "    model = model_save(model)\n",
    "    print(\"Model Saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored like this:\n",
    "['SubjectID', 'Device','ActivityID','TrialNo','Acc','Gyr']\n",
    "- The label (output) is the ActivityID\n",
    "    - These labels are specified in function get_activity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection(num = 2):\n",
    "    #1: runfall, downSit, freeFall, runSit, walkFall, walkSit\n",
    "    #2: ActivityID labels\n",
    "    # create a new directory called 'normalized', ignore it if it already exists\n",
    "    os.makedirs('./normalized', exist_ok=True)\n",
    "\n",
    "    # import pickle file\n",
    "    df = pd.read_pickle('FallAllD.pkl')\n",
    "    # convert all columns to float32\n",
    "    df = df.astype(np.float32)\n",
    "\n",
    "    #display df\n",
    "    #print(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def data_normalize(df): #CHECK IF NEEDED\n",
    "    # for each column in 'Accerlation', 'AccelerationY', 'AccelerationZ',\n",
    "    # add a new column that is the fft of the original column\n",
    "    for c in ['Acc']:\n",
    "        lst = []\n",
    "        for i in c:\n",
    "           lst.append(np.fft.fft(i))\n",
    "        df[c + '_fft'] = lst\n",
    "\n",
    "    # debug purposes\n",
    "    for c in df.columns:\n",
    "        print(c)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def data_categorical(df): #TODO: CHECK IF NEEDED\n",
    "    # convert column 'Device' to numerical data\n",
    "    df['Device'] = df['Device'].astype('category')\n",
    "    df['Device'] = df['Device'].cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "def data_split(df):\n",
    "    # split the data into features and labels\n",
    "    x = df[['SubjectID', 'Device','TrialNo','Acc_x','Acc_y','Acc_z', 'Gyr_x', 'Gyr_y', 'Gyr_z']]\n",
    "    y = df['ActivityID']\n",
    "\n",
    "    #Spliting Data\n",
    "    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x,y,test_size = 0.3)\n",
    "\n",
    "    # reshape training and testing data\n",
    "    y_train = np.array(y_train).reshape(-1,1) # (-1,1) because our data has a single feature, and a 'n' amount of rows\n",
    "    y_test = np.array(y_test).reshape(-1,1) # (-1,1) bec  ause our data has a single feature, and a 'n' amount of rows\n",
    "    x_test = np.array(x_test).reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def model_create(x_train):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(128, input_shape=(x_train.shape[1], 1)))\n",
    "  model.add(Dropout(0.2))\n",
    "  # there are 135 different activities, so we need 135 neurons in the output layer\n",
    "  model.add(Dense(136, activation='softmax'))\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def train_and_accurary_model(model, x_train, x_test, y_train, y_test):\n",
    "  #reshape the features for the LSTM layer (I REALLY WANT TO FIX UGHHHG)\n",
    "  x_train = np.array(x_train).reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "\n",
    "  # train the model\n",
    "  model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "  # evaluate the model\n",
    "  test_loss, test_Acc = model.evaluate(x_test, y_test)\n",
    "  print('Test accuracy:', test_Acc)\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "def model_save(model):\n",
    "  # save the model\n",
    "  model.save('saved_model/my_model')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m head(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'head' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging function\n",
    "def data_plot(df):\n",
    "    #debugging function. does not need to be called, or return anything\n",
    "\n",
    "    # plot the first 100 column 'AccelerationX' and 'AccelerationX_fft'\n",
    "    df[['Acc'[0]], ['Acc'[1]], ['Acc'[2]]].iloc[:100].plot()\n",
    "\n",
    "    # Versus Head (the same)\n",
    "    #df[['AccelerationX_fft', 'AccelerationY_fft', 'AccelerationZ_fft']].head(100).plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(num = 0):\n",
    "    # a dictionary that maps the case number (ActivityID) with stirng label\n",
    "    activity_dict = {\n",
    "    101: 'Fall F, walking, trip',\n",
    "    102: 'Fall F, walking, trip, rec.',\n",
    "    103: 'Fall F, walking, slip',\n",
    "    104: 'Fall F, walking, slip, rec.',\n",
    "    105: 'Fall F, walking, slip, rot.',\n",
    "    106: 'Fall F, walking, slip, rot., rec.',\n",
    "    107: 'Fall B, walking, slip',\n",
    "    108: 'Fall B, walking, slip, rec.',\n",
    "    109: 'Fall B, walking, slip, rot.',\n",
    "    110: 'Fall B, walking, slip rot., rec.',\n",
    "    111: 'Fall F, walking, syncope',\n",
    "    112: 'Fall B, walking, syncope',\n",
    "    113: 'Fall L, walking, syncope',\n",
    "    114: 'Fall, syncope, table',\n",
    "    115: 'Fall F, try sit',\n",
    "    116: 'Fall F, try sit, rec.',\n",
    "    117: 'Fall B, try sit',\n",
    "    118: 'Fall B, try sit, rec.',\n",
    "    119: 'Fall L, try sit',\n",
    "    120: 'Fall L, try sit, rec.',\n",
    "    121: 'Fall F, jog, trip',\n",
    "    122: 'Fall F, jog, trip, rec.',\n",
    "    123: 'Fall F, jog, slip',\n",
    "    124: 'Fall F, jog, slip, rev.',\n",
    "    125: 'Fall F, jog, slip, rot.',\n",
    "    126: 'Fall F, jog, slip, rot., rec.',\n",
    "    127: 'Fall L, bed',\n",
    "    128: 'Fall L, bed, rec.',\n",
    "    129: 'Fall F, chair, syncope',\n",
    "    130: 'Fall B, chair, syncope',\n",
    "    131: 'Fall L, chair, syncope',\n",
    "    132: 'Fall F, syncope',\n",
    "    133: 'Fall B, syncope',\n",
    "    134: 'Fall L, syncope',\n",
    "    135: 'Fall, syncope, slide over a wall',\n",
    "    1: 'Start clap hands',\n",
    "    2: 'Clap hands',\n",
    "    3: 'Stop clap hands',\n",
    "    4: 'Clap hands 1',\n",
    "    5: 'Start wave hands',\n",
    "    6: 'wave hands',\n",
    "    7: 'Stop wave hands',\n",
    "    8: 'Raising hand up',\n",
    "    9: 'Moving hand down',\n",
    "    10: 'Move hand up -> down',\n",
    "    11: 'Hand shaking',\n",
    "    12: 'Beating a table',\n",
    "    13: 'Sitting down',\n",
    "    14: 'Standing up',\n",
    "    15: 'Fail to stand up',\n",
    "    16: 'Lying down',\n",
    "    17: 'Turning while lying',\n",
    "    18: 'Rising up',\n",
    "    19: 'Start walking',\n",
    "    20: 'Walking slowly',\n",
    "    21: 'Stop walking',\n",
    "    22: 'Walking quickly',\n",
    "    23: 'Stumbling',\n",
    "    24: 'Jogging slowly',\n",
    "    25: 'Jogging quickly',\n",
    "    26: 'Jumping slightly',\n",
    "    27: 'Jumping strongly',\n",
    "    28: 'B...'\n",
    "    }\n",
    "    return activity_dict[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_collection(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data collected\n",
      "(11971400, 10)\n",
      "(8379980, 9) (8379980, 1)\n",
      "(3591420, 9, 1) (3591420, 1)\n",
      "Data Split Done\n",
      "Model Created\n",
      "Epoch 1/10\n",
      "  5878/261875 [..............................] - ETA: 20:22 - loss: 3.8565 - accuracy: 0.0746"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m model \u001b[39m=\u001b[39m model_create(x_train)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel Created\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m model \u001b[39m=\u001b[39m train_and_accurary_model(model, x_train, x_test, y_train, y_test)\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel Trained\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m model \u001b[39m=\u001b[39m model_save(model)\n",
      "Cell \u001b[0;32mIn[70], line 69\u001b[0m, in \u001b[0;36mtrain_and_accurary_model\u001b[0;34m(model, x_train, x_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     66\u001b[0m x_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(x_train)\u001b[39m.\u001b[39mreshape(x_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], x_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(x_test, y_test))\n\u001b[1;32m     71\u001b[0m \u001b[39m# evaluate the model\u001b[39;00m\n\u001b[1;32m     72\u001b[0m test_loss, test_Acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(x_test, y_test)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# convert all columns to float32\n",
    "df = df.astype('float32')\n",
    "#df = data_normalize(df) #Don't know if I need to normalize\n",
    "df = data_categorical(df) #check for 'Device'\n",
    "print('data collected')\n",
    "print(df.shape)\n",
    "x_train, x_test, y_train, y_test = data_split(df)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "print(\"Data Split Done\")\n",
    "model = model_create(x_train)\n",
    "print(\"Model Created\")\n",
    "model = train_and_accurary_model(model, x_train, x_test, y_train, y_test)\n",
    "print(\"Model Trained\")\n",
    "model = model_save(model)\n",
    "print(\"Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "919ada50126e9b7679f5f5dbe18aa91f971b460080d3011effa906ebff41bb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
