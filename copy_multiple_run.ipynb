{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN IF YOU ARE NOT USING WINDOWS AND/OR NOT USING ANACONDA\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install os-sys\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install \"tensorflow-gpu<2.10\"\n",
    "%pip install \"tensorflow<2.10\"\n",
    "%pip install \"keras<2.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are on Google Colab run this\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn \n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "import sklearn.model_selection\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection():\n",
    "    # create directory normalized if it does not exist\n",
    "    os.makedirs('./normalized', exist_ok=True) \n",
    "\n",
    "    # import csv file\n",
    "    # ON GOOGLE COLAB\n",
    "        # read csv file from your google drive. find the file in your drive and copy the path and replace\n",
    "        # the path in the read_csv function with the path to your file\n",
    "    df = pd.read_csv('FallAllD2.csv')\n",
    "    # convert all columns to float32\n",
    "    df = df.astype('float32')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all():\n",
    "    df = data_collection()\n",
    "    df = df[df['Device'] == 2]\n",
    "    for i in range(0,7):\n",
    "        x_train, x_test, y_train, y_test = data_split(df, i)\n",
    "        print(x_train.shape, y_train.shape)\n",
    "        print(x_test.shape, y_test.shape)\n",
    "\n",
    "        # Create and Train Model\n",
    "        model = model_create(x_train)\n",
    "        print(\"Model Created\")\n",
    "        model = train_and_accurary_model(model, x_train, x_test, y_train, y_test)\n",
    "        print(\"Model Trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_label(df):\n",
    "    # add a new column called \"IsFall\" that is 1 if the ActivityID > 100, and 0 if it is not\n",
    "    df['IsFall'] = df['ActivityID'].apply(lambda x: 1 if x > 100 else 0)\n",
    "    return df\n",
    "\n",
    "def data_split(df, num):\n",
    "    df = data_label(df)\n",
    "    # split the data into features and labels\n",
    "    if num == 0: x = df[['Device','Acc_x','Acc_y','Acc_z', 'Gyr_x', 'Gyr_y', 'Gyr_z', 'Bar_x', 'Bar_y']]\n",
    "    elif num == 1: x = df[['Device','Acc_x','Acc_y','Acc_z', 'Gyr_x', 'Gyr_y', 'Gyr_z']]\n",
    "    elif num == 2: x = df[['Device','Gyr_x', 'Gyr_y', 'Gyr_z', 'Bar_x', 'Bar_y']]\n",
    "    elif num == 3: x = df[['Device','Acc_x','Acc_y','Acc_z', 'Bar_x', 'Bar_y']]\n",
    "    elif num == 4: x = df[['Device','Acc_x','Acc_y','Acc_z']]\n",
    "    elif num == 5: x = df[['Device','Gyr_x', 'Gyr_y', 'Gyr_z']]\n",
    "    elif num == 6: x = df[['Device','Bar_x', 'Bar_y']]\n",
    "    y = df['IsFall']\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "    x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "    \n",
    "    #Spliting Data\n",
    "    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x,y,test_size = 0.2)\n",
    "    print('x y shape: ', x_train.shape, y_train.shape)\n",
    "\n",
    "    # the following is not needed, the reshape is already done\n",
    "    if False:\n",
    "        # reshape training and testing data\n",
    "        y_train = np.array(y_train).reshape(-1,1) # (-1,1) because our data has a single feature, and a 'n' amount of rows\n",
    "        y_test = np.array(y_test).reshape(-1,1) # (-1,1) bec  ause our data has a single feature, and a 'n' amount of rows\n",
    "        #reshape the features for the LSTM layer (I REALLY WANT TO FIX UGHHHG)\n",
    "        steps = x_test.shape[1]\n",
    "        x_train = np.array(x_train).reshape(x_train.shape[0], steps, 1)\n",
    "        x_test = np.array(x_test).reshape(x_test.shape[0], steps, 1)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def model_create(x_train):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(512, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "  model.add(Dropout(0.2))\n",
    "  \n",
    "  # add a Flatten layer using x_train as input shape\n",
    "  #model.add(Flatten(input_shape=x_train.shape[1:]))\n",
    "\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dropout(0.3))\n",
    "\n",
    "  # for activity classification, we need 136 neurons in the output layer and categorical crossentropy as the loss function\n",
    "  #model.add(Dense(136, activation='softmax'))\n",
    "  #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "  # for IsFall classification, we need 1 neuron in the output layer and binary crossentropy as the loss function\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def train_and_accurary_model(model, x_train, x_test, y_train, y_test):\n",
    "\n",
    "  # train the model\n",
    "  model.fit(x_train, y_train, epochs=10, batch_size=256, validation_split=0.1)\n",
    "\n",
    "  # evaluate the model\n",
    "  test_loss, test_Acc = model.evaluate(x_test, y_test)\n",
    "  print('Test accuracy:', test_Acc)\n",
    "  model.summary()\n",
    "  print(\"Confusion Matrix\")\n",
    "  y_pred = model.predict(x_test)\n",
    "  y_pred = (y_pred > 0.5)\n",
    "  confusion_mtx = confusion_matrix(y_test, y_pred)\n",
    "  confusion_mtx_percent = confusion_mtx / confusion_mtx.sum(axis=1)[:, np.newaxis]\n",
    "  print(confusion_mtx_percent)\n",
    "\n",
    "  # Save Model to Local Machine\n",
    "  file_name = 'model' + str(i) + '_device2_aggregate.h5'\n",
    "\n",
    "  model.save(file_name)\n",
    "  from google.colab import files\n",
    "  files.download(file_name)  # Download to local machine\n",
    "  print(\"Model downloaded to local machine\")\n",
    "\n",
    "  # Save Model to Google Drive\n",
    "  model.save('/content/drive/My Drive/' + file_name )\n",
    "  print(\"ModelÂ saved to Google Drive\")\n",
    "  \n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model from folder all_models. take model summaries and put them into a txt file. download txt file to local machine\n",
    "def get_model_summaries():\n",
    "    import os\n",
    "    from keras.models import load_model\n",
    "    from keras.utils.vis_utils import plot_model\n",
    "    from keras.utils import print_summary\n",
    "    \n",
    "    # get all files in the folder\n",
    "    files = os.listdir('all models')\n",
    "    print(files)\n",
    "    \n",
    "    # open a file to write the model summaries to\n",
    "    f = open(\"model_summaries.txt\", \"w\")\n",
    "    \n",
    "    # loop through all files in the folder\n",
    "    for file in files:\n",
    "        # load the model\n",
    "        model = load_model('all_models/' + file)\n",
    "        # get the model summary\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        # get the model plot\n",
    "        plot_model(model, to_file='model_plots/' + file + '.png', show_shapes=True, show_layer_names=True)\n",
    "        # write a line break\n",
    "        f.write('\\n\\n')\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that are not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(num = 0):\n",
    "    # a dictionary that maps the case number (ActivityID) with stirng label\n",
    "    activity_dict = {\n",
    "    101: 'Fall F, walking, trip',\n",
    "    102: 'Fall F, walking, trip, rec.',\n",
    "    103: 'Fall F, walking, slip',\n",
    "    104: 'Fall F, walking, slip, rec.',\n",
    "    105: 'Fall F, walking, slip, rot.',\n",
    "    106: 'Fall F, walking, slip, rot., rec.',\n",
    "    107: 'Fall B, walking, slip',\n",
    "    108: 'Fall B, walking, slip, rec.',\n",
    "    109: 'Fall B, walking, slip, rot.',\n",
    "    110: 'Fall B, walking, slip rot., rec.',\n",
    "    111: 'Fall F, walking, syncope',\n",
    "    112: 'Fall B, walking, syncope',\n",
    "    113: 'Fall L, walking, syncope',\n",
    "    114: 'Fall, syncope, table',\n",
    "    115: 'Fall F, try sit',\n",
    "    116: 'Fall F, try sit, rec.',\n",
    "    117: 'Fall B, try sit',\n",
    "    118: 'Fall B, try sit, rec.',\n",
    "    119: 'Fall L, try sit',\n",
    "    120: 'Fall L, try sit, rec.',\n",
    "    121: 'Fall F, jog, trip',\n",
    "    122: 'Fall F, jog, trip, rec.',\n",
    "    123: 'Fall F, jog, slip',\n",
    "    124: 'Fall F, jog, slip, rev.',\n",
    "    125: 'Fall F, jog, slip, rot.',\n",
    "    126: 'Fall F, jog, slip, rot., rec.',\n",
    "    127: 'Fall L, bed',\n",
    "    128: 'Fall L, bed, rec.',\n",
    "    129: 'Fall F, chair, syncope',\n",
    "    130: 'Fall B, chair, syncope',\n",
    "    131: 'Fall L, chair, syncope',\n",
    "    132: 'Fall F, syncope',\n",
    "    133: 'Fall B, syncope',\n",
    "    134: 'Fall L, syncope',\n",
    "    135: 'Fall, syncope, slide over a wall',\n",
    "    1: 'Start clap hands',\n",
    "    2: 'Clap hands',\n",
    "    3: 'Stop clap hands',\n",
    "    4: 'Clap hands 1',\n",
    "    5: 'Start wave hands',\n",
    "    6: 'wave hands',\n",
    "    7: 'Stop wave hands',\n",
    "    8: 'Raising hand up',\n",
    "    9: 'Moving hand down',\n",
    "    10: 'Move hand up -> down',\n",
    "    11: 'Hand shaking',\n",
    "    12: 'Beating a table',\n",
    "    13: 'Sitting down',\n",
    "    14: 'Standing up',\n",
    "    15: 'Fail to stand up',\n",
    "    16: 'Lying down',\n",
    "    17: 'Turning while lying',\n",
    "    18: 'Rising up',\n",
    "    19: 'Start walking',\n",
    "    20: 'Walking slowly',\n",
    "    21: 'Stop walking',\n",
    "    22: 'Walking quickly',\n",
    "    23: 'Stumbling',\n",
    "    24: 'Jogging slowly',\n",
    "    25: 'Jogging quickly',\n",
    "    26: 'Jumping slightly',\n",
    "    27: 'Jumping strongly',\n",
    "    28: 'B...'\n",
    "    }\n",
    "    return activity_dict[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging function\n",
    "def data_plot(df):\n",
    "    #debugging function. does not need to be called, or return anything\n",
    "\n",
    "    # plot the first 100 column 'AccelerationX' and 'AccelerationX_fft'\n",
    "    df[['Acc'[0]], ['Acc'[1]], ['Acc'[2]]].iloc[:100].plot()\n",
    "\n",
    "    # Versus Head (the same)\n",
    "    #df[['AccelerationX_fft', 'AccelerationY_fft', 'AccelerationZ_fft']].head(100).plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalize(df): #Not in use\n",
    "    # for each column in 'Accerlation', 'AccelerationY', 'AccelerationZ',\n",
    "    # add a new column that is the fft of the original column\n",
    "    for c in ['AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ']:\n",
    "        df[c + '_fft'] = np.fft.fft(df[c])\n",
    "        \n",
    "    return df\n",
    "\n",
    "def data_categorical(df): #Not in use\n",
    "    # convert column 'Device' to numerical data\n",
    "    df['Device'] = df['Device'].astype('category')\n",
    "    df['Device'] = df['Device'].cat.codes\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "919ada50126e9b7679f5f5dbe18aa91f971b460080d3011effa906ebff41bb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
