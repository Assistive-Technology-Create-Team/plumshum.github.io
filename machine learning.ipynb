{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) that is designed to handle the problem of vanishing gradient in traditional RNNs. LSTM networks can learn long-term dependencies in sequential data and are widely used in various applications such as speech recognition, language modeling, and time series forecasting.\n",
    "\n",
    "The key feature of an LSTM cell is its ability to selectively remember and forget information using its memory cell and gates. The memory cell stores the previous hidden state and carries it forward in time. The input gate determines how much new information is allowed into the memory cell, and the forget gate decides how much of the previous memory to retain or forget. Finally, the output gate controls how much of the current memory cell state is used to generate the output. The gates are controlled by sigmoid and element-wise multiplication operations, and the memory cell is updated using a tanh function.\n",
    "\n",
    "To train an LSTM model, we need to define the architecture, loss function, and optimization method. The input to the model is a sequence of vectors, and the output is a sequence of predicted values. The architecture can have multiple LSTM layers with varying hidden units, dropout, and activation functions. The loss function can be Mean Squared Error (MSE) or Binary Cross Entropy (BCE), depending on the type of problem. The optimization method can be Stochastic Gradient Descent (SGD), Adam, or other variations.\n",
    "\n",
    "To make our LSTM model have the highest accuracy, we can follow the following experimental procedure:\n",
    "\n",
    "1. Preprocess the data: The input data should be normalized, and the sequences should be padded or truncated to a fixed length. We can also use techniques like data augmentation or feature engineering to enhance the data quality.\n",
    "\n",
    "2. Split the data: We need to split the data into training, validation, and test sets. The training set is used to update the model parameters, the validation set is used to monitor the model performance and avoid overfitting, and the test set is used to evaluate the final model accuracy.\n",
    "\n",
    "3. Define the architecture: We can start with a simple LSTM model and gradually increase the complexity by adding more layers or hidden units. We can also use techniques like early stopping, learning rate scheduling, or batch normalization to improve the model performance.\n",
    "\n",
    "4. Train the model: We can use the Adam optimizer with a learning rate of 0.001, and a batch size of 32. We can train the model for 50 epochs and monitor the validation loss to avoid overfitting. We can also use techniques like gradient clipping or weight decay to prevent exploding gradients.\n",
    "\n",
    "5. Evaluate the model: We can use the test set to evaluate the final model accuracy. We can compute metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or Mean Absolute Error (MAE) to assess the model performance. We can also visualize the predicted values against the ground truth to gain insights into the model behavior.\n",
    "\n",
    "6. By following these steps, we can create an LSTM model that achieves high accuracy and generalizes well to new data. The key to success is to experiment with different architectures, optimization methods, and hyperparameters and choose the ones that work best for our specific problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model\n",
    "1. Make sure all packages are installed (WINDOWS)\n",
    "\n",
    "*    pip install numpy\n",
    "*   pip install pandas\n",
    "*   pip install os-sys\n",
    "*    pip install matplotlib\n",
    "*   pip install seaborn\n",
    "*   pip install scikit-learn\n",
    "*   pip install keras\n",
    "*   pip install tensorflow \n",
    "\n",
    "1. Make sure all packages are installed (MACS AND/OR ANACONDA ENVIRONMENT)\n",
    "*   conda  install numpy\n",
    "*   conda  install pandas\n",
    "*   conda  install os-sys\n",
    "*   conda  install matplotlib\n",
    "*   conda  install seaborn\n",
    "*   conda  install scikit-learn\n",
    "*   conda  install keras\n",
    "*   conda  install tensorflow \n",
    "\n",
    "Links:\n",
    "\n",
    "Kaggle Data: https://www.kaggle.com/harnoor343/fall-detection-accelerometer-data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Issues:\n",
    " * can I reshape before putting in LSTM layer :(\n",
    "\n",
    "Todo:\n",
    "* understand acceleration data comepared to previosu acceleration data\n",
    "* normalize it?\n",
    "* let's use fall data only first cases (), then all the cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN IF YOU ARE NOT USING WINDOWS AND/OR NOT USING ANACONDA\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install os-sys\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install \"tensorflow-gpu<2.10\"\n",
    "%pip install \"tensorflow<2.10\"\n",
    "%pip install \"keras<2.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are on Google Colab run this\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn \n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "import sklearn.model_selection\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored like this:\n",
    "['SubjectID', 'Device','ActivityID','TrialNo','Acc_x', 'Acc_y', 'Acc_z\n",
    ", Gyr_x', 'Gyr_y', 'Gyr_z']\n",
    "- The label (output) is the ActivityID\n",
    "    - These labels are specified in function get_activity()\n",
    "    - labels <= 100 are non-fall activities. 100 < labels < 135 are fall activities \n",
    "    - output is either 'IsFall' or no label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection():\n",
    "    # create directory normalized if it does not exist\n",
    "    os.makedirs('./normalized', exist_ok=True) \n",
    "\n",
    "    # import csv file\n",
    "    # ON GOOGLE COLAB\n",
    "        # read csv file from your google drive. find the file in your drive and copy the path and replace\n",
    "        # the path in the read_csv function with the path to your file\n",
    "    df = pd.read_csv('FallAllD2.csv')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data collected\n",
      "(31439800, 12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# convert all columns to float32\n",
    "df = df.astype('float32')\n",
    "#df = data_normalize(df) #Don't know if I need to normalize\n",
    "print('data collected')\n",
    "print(df.shape)\n",
    "original_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_df.copy()\n",
    "print(df.head(5))\n",
    "# only keep `SubjectID` == 1\n",
    "#df = df[df['SubjectID'] == 1]\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0.1  Unnamed: 0  SubjectID  Device  ActivityID  TrialNo   Acc_x  \\\n",
      "0           0.0         0.0        1.0     1.0       101.0      1.0  4974.0   \n",
      "\n",
      "   Acc_y  Acc_z  Gyr_x  Gyr_y  Gyr_z  \n",
      "0  803.0  684.0  581.0  184.0  204.0  \n",
      "x y shape:  (25151840, 1, 7) (25151840,)\n",
      "(25151840, 1, 7) (25151840,)\n",
      "(6287960, 1, 7) (6287960,)\n",
      "Data Split Done\n"
     ]
    }
   ],
   "source": [
    "def data_label(df):\n",
    "    # add a new column called \"IsFall\" that is 1 if the ActivityID > 100, and 0 if it is not\n",
    "    df['IsFall'] = df['ActivityID'].apply(lambda x: 1 if x > 100 else 0)\n",
    "    return df\n",
    "\n",
    "def data_split(df):\n",
    "    df = data_label(df)\n",
    "    # split the data into features and labels\n",
    "    x = df[['Device','Acc_x','Acc_y','Acc_z', 'Gyr_x', 'Gyr_y', 'Gyr_z']]\n",
    "    #y = df['ActivityID']\n",
    "    y = df['IsFall']\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "    x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "    \n",
    "    #Spliting Data\n",
    "    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x,y,test_size = 0.2)\n",
    "    print('x y shape: ', x_train.shape, y_train.shape)\n",
    "\n",
    "    # the following is not needed, the reshape is already done\n",
    "    if False:\n",
    "        # reshape training and testing data\n",
    "        y_train = np.array(y_train).reshape(-1,1) # (-1,1) because our data has a single feature, and a 'n' amount of rows\n",
    "        y_test = np.array(y_test).reshape(-1,1) # (-1,1) bec  ause our data has a single feature, and a 'n' amount of rows\n",
    "        #reshape the features for the LSTM layer (I REALLY WANT TO FIX UGHHHG)\n",
    "        steps = x_test.shape[1]\n",
    "        x_train = np.array(x_train).reshape(x_train.shape[0], steps, 1)\n",
    "        x_test = np.array(x_test).reshape(x_test.shape[0], steps, 1)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "df = original_df.copy()\n",
    "# only keep the columns 'Device' = 2\n",
    "df = df[df['Device'] == 2]\n",
    "print(df.head(1))\n",
    "x_train, x_test, y_train, y_test = data_split(df)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "print(\"Data Split Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def model_create(x_train):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(512, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "  model.add(Dropout(0.2))\n",
    "  \n",
    "  # add a Flatten layer using x_train as input shape\n",
    "  #model.add(Flatten(input_shape=x_train.shape[1:]))\n",
    "\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dropout(0.3))\n",
    "\n",
    "  # for activity classification, we need 136 neurons in the output layer and categorical crossentropy as the loss function\n",
    "  #model.add(Dense(136, activation='softmax'))\n",
    "  #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "  # for IsFall classification, we need 1 neuron in the output layer and binary crossentropy as the loss function\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def train_and_accurary_model(model, x_train, x_test, y_train, y_test):\n",
    "\n",
    "  # train the model\n",
    "  model.fit(x_train, y_train, epochs=10, batch_size=256, validation_split=0.1)\n",
    "\n",
    "  # evaluate the model\n",
    "  test_loss, test_Acc = model.evaluate(x_test, y_test)\n",
    "  print('Test accuracy:', test_Acc)\n",
    "  model.summary()\n",
    "  print(\"Confusion Matrix\")\n",
    "  y_pred = model.predict(x_test)\n",
    "  y_pred = (y_pred > 0.5)\n",
    "  confusion_mtx = confusion_matrix(y_test, y_pred)\n",
    "  confusion_mtx_percent = confusion_mtx / confusion_mtx.sum(axis=1)[:, np.newaxis]\n",
    "  print(confusion_mtx_percent)\n",
    "\n",
    "  # Save Model Summary and Confusion Matrix into a text file\n",
    "  with open(file_name + '.txt', 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    f.write('Confusion Matrix \\n')\n",
    "    f.write(str(confusion_mtx_percent))\n",
    "  files.download(file_name + '.txt')  # Download to local machine\n",
    "  \n",
    "  return model\n",
    "\n",
    "# Create and Train Model\n",
    "model = model_create(x_train)\n",
    "print(\"Model Created\")\n",
    "model = train_and_accurary_model(model, x_train, x_test, y_train, y_test)\n",
    "print(\"Model Trained\")\n",
    "\n",
    "# Save Model to Local Machine\n",
    "file_name = 'model_device2_aggregate.h5'\n",
    "model.save(file_name)\n",
    "from google.colab import files\n",
    "files.download(file_name)  # Download to local machine\n",
    "print(\"Model downloaded to local machine\")\n",
    "\n",
    "# Save Model to Google Drive\n",
    "model.save('/content/drive/My Drive/' + file_name )\n",
    "print(\"Model saved to Google Drive\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that are not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(num = 0):\n",
    "    # a dictionary that maps the case number (ActivityID) with stirng label\n",
    "    activity_dict = {\n",
    "    101: 'Fall F, walking, trip',\n",
    "    102: 'Fall F, walking, trip, rec.',\n",
    "    103: 'Fall F, walking, slip',\n",
    "    104: 'Fall F, walking, slip, rec.',\n",
    "    105: 'Fall F, walking, slip, rot.',\n",
    "    106: 'Fall F, walking, slip, rot., rec.',\n",
    "    107: 'Fall B, walking, slip',\n",
    "    108: 'Fall B, walking, slip, rec.',\n",
    "    109: 'Fall B, walking, slip, rot.',\n",
    "    110: 'Fall B, walking, slip rot., rec.',\n",
    "    111: 'Fall F, walking, syncope',\n",
    "    112: 'Fall B, walking, syncope',\n",
    "    113: 'Fall L, walking, syncope',\n",
    "    114: 'Fall, syncope, table',\n",
    "    115: 'Fall F, try sit',\n",
    "    116: 'Fall F, try sit, rec.',\n",
    "    117: 'Fall B, try sit',\n",
    "    118: 'Fall B, try sit, rec.',\n",
    "    119: 'Fall L, try sit',\n",
    "    120: 'Fall L, try sit, rec.',\n",
    "    121: 'Fall F, jog, trip',\n",
    "    122: 'Fall F, jog, trip, rec.',\n",
    "    123: 'Fall F, jog, slip',\n",
    "    124: 'Fall F, jog, slip, rev.',\n",
    "    125: 'Fall F, jog, slip, rot.',\n",
    "    126: 'Fall F, jog, slip, rot., rec.',\n",
    "    127: 'Fall L, bed',\n",
    "    128: 'Fall L, bed, rec.',\n",
    "    129: 'Fall F, chair, syncope',\n",
    "    130: 'Fall B, chair, syncope',\n",
    "    131: 'Fall L, chair, syncope',\n",
    "    132: 'Fall F, syncope',\n",
    "    133: 'Fall B, syncope',\n",
    "    134: 'Fall L, syncope',\n",
    "    135: 'Fall, syncope, slide over a wall',\n",
    "    1: 'Start clap hands',\n",
    "    2: 'Clap hands',\n",
    "    3: 'Stop clap hands',\n",
    "    4: 'Clap hands 1',\n",
    "    5: 'Start wave hands',\n",
    "    6: 'wave hands',\n",
    "    7: 'Stop wave hands',\n",
    "    8: 'Raising hand up',\n",
    "    9: 'Moving hand down',\n",
    "    10: 'Move hand up -> down',\n",
    "    11: 'Hand shaking',\n",
    "    12: 'Beating a table',\n",
    "    13: 'Sitting down',\n",
    "    14: 'Standing up',\n",
    "    15: 'Fail to stand up',\n",
    "    16: 'Lying down',\n",
    "    17: 'Turning while lying',\n",
    "    18: 'Rising up',\n",
    "    19: 'Start walking',\n",
    "    20: 'Walking slowly',\n",
    "    21: 'Stop walking',\n",
    "    22: 'Walking quickly',\n",
    "    23: 'Stumbling',\n",
    "    24: 'Jogging slowly',\n",
    "    25: 'Jogging quickly',\n",
    "    26: 'Jumping slightly',\n",
    "    27: 'Jumping strongly',\n",
    "    28: 'B...'\n",
    "    }\n",
    "    return activity_dict[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging function\n",
    "def data_plot(df):\n",
    "    #debugging function. does not need to be called, or return anything\n",
    "\n",
    "    # plot the first 100 column 'AccelerationX' and 'AccelerationX_fft'\n",
    "    df[['Acc'[0]], ['Acc'[1]], ['Acc'[2]]].iloc[:100].plot()\n",
    "\n",
    "    # Versus Head (the same)\n",
    "    #df[['AccelerationX_fft', 'AccelerationY_fft', 'AccelerationZ_fft']].head(100).plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalize(df): #Not in use\n",
    "    # for each column in 'Accerlation', 'AccelerationY', 'AccelerationZ',\n",
    "    # add a new column that is the fft of the original column\n",
    "    for c in ['AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ']:\n",
    "        df[c + '_fft'] = np.fft.fft(df[c])\n",
    "        \n",
    "    return df\n",
    "\n",
    "def data_categorical(df): #Not in use\n",
    "    # convert column 'Device' to numerical data\n",
    "    df['Device'] = df['Device'].astype('category')\n",
    "    df['Device'] = df['Device'].cat.codes\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "919ada50126e9b7679f5f5dbe18aa91f971b460080d3011effa906ebff41bb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
